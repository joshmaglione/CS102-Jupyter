{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/joshmaglione/CS102-Jupyter/main?labpath=.%2FWeek07.ipynb) \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/joshmaglione/CS102-Jupyter/blob/main/Week07.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> \n",
    "\n",
    "[View on GitHub](https://github.com/joshmaglione/CS102-Jupyter/blob/main/Week07.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Working with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we discussed the three main data structures `pandas` brings.\n",
    "\n",
    "Now we will discuss how to manipulate the primary object: DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in UN data about Ireland from two files sources: \n",
    "- `data/01_below_poverty.csv`\n",
    "- `data/07_renewable_energy.csv`\n",
    "\n",
    "(This was obtained from the [United Nations' SDG Country Profile Page](https://unstats.un.org/sdgs/dataportal/countryprofiles/IRL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has lots of information that we don't need, so our goal is to produce one DataFrame with the information we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can [read more](https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html#copy-on-write) about their upcoming changes to `pandas 3.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Poverty Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work through some of the basics with the below poverty data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/01_below_poverty.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the columns look irrelevant. Let's look to keep the columns\n",
    "- \"TimePeriod\"\n",
    "- \"Value\"\n",
    "- \"Time_Detail\"\n",
    "- \"Age\"\n",
    "- \"Location\"\n",
    "- \"Sex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[[\n",
    "\t\"TimePeriod\",\n",
    "\t\"Value\",\n",
    "\t\"Time_Detail\",\n",
    "\t\"Age\",\n",
    "\t\"Location\",\n",
    "\t\"Sex\"\n",
    "]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's weird that `\"TimePeriod\"` is a float. Let's change this to an int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.astype({\"TimePeriod\" : \"i\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean our data before we can manipulate it. Let's remove the rows where `\"TimePeriod\"` is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna(subset=\"TimePeriod\")\n",
    "df1 = df1.astype({\"TimePeriod\" : \"i\"})\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to span the entirity of this data set. Let's just take the years 2005 to 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do this with the [`query`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html) command. \n",
    "\n",
    "At a basic level it takes a string that appropriately masks part of the dataframe.\n",
    "\n",
    "By default, `query` is a copy, unlike slicing in `ndarray` which yields a view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_check = df1.query(\"2005 <= TimePeriod <= 2020\")\n",
    "df1.at[22, \"TimePeriod\"] = 1000     # Index 22 is the first row with 2005\n",
    "df1.loc[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1_check\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"Time_Detail\"` column looks irrelevant. \n",
    "\n",
    "Let's look at all the values and the number of time they arise with [`value_counts`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Time_Detail\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop([\"Time_Detail\"], axis=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last four columns are potentially irrelevant. Let's see what the possibilities are for \n",
    "- \"Age\"\n",
    "- \"Location\"\n",
    "- \"Sex\"\n",
    "\n",
    "We want the most inclusive options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the rows for one particular year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.query(\"2011 == TimePeriod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set has lots of granularity. Let's just take the coarse, general information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do this by chaining three queries. \n",
    "\n",
    "In comments, we have an equivalent version, but it is generally considered good practice to chain as follows.\n",
    "\n",
    "*This is just for aesthetic and reabability reasons.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df1[\n",
    "# \t(df1[\"Age\"] == \"ALLAGE\") & \n",
    "# \t(df1[\"Location\"] == \"ALLAREA\") &\n",
    "# \t(df1[\"Sex\"] == \"BOTHSEX\")\n",
    "# ]\n",
    "df1 = (df1\n",
    "    .query(\"Age == 'ALLAGE'\")\n",
    "    .query(\"Location == 'ALLAREA'\")\n",
    "    .query(\"Sex == 'BOTHSEX'\")\n",
    ")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the last three columns are constant, we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop([\"Age\", \"Location\", \"Sex\"], axis=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current index is also irrelevant. Let's convert `\"TimePeriod\"` to our index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.set_index(\"TimePeriod\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's change `\"Value\"` to `\"Below Poverty (%)\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns={\"Value\" : \"Below Poverty (%)\"})\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedrun: Renewable Energy Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will basically do the same steps as above, but all at once. See if you can follow along line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"data/07_renewable_energy.csv\")\n",
    "df2 = df2[[\"TimePeriod\", \"Value\"]]\n",
    "df2 = df2.dropna()\n",
    "df2 = df2.astype({\"TimePeriod\" : \"i\"})\n",
    "df2 = df2.set_index(\"TimePeriod\")\n",
    "df2 = df2.loc[\"2005\":\"2020\"]\n",
    "df2 = df2.rename(columns={\"Value\" : \"Renewable Energy Share (%)\"})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From two to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our two DataFrames have the same index, we can concatenate them in `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], axis=1)\t\t# Merging our two DataFrames\n",
    "df.index.names = [\"Year\"]\t\t\t\t# Renaming the index\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get some quick statistics on our data.\n",
    "\n",
    "Using the `describe` method, we get the average and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detour: visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll discuss this more later, but we can now plot the DataFrame on a set of axes with `Matplotlib` (working in the background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ufuncs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the `NumPy` UFuncs can be applied to DataFrames, provided they are applied to appropriate numerical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's generate some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "# Generate some random data with N + 1 rows\n",
    "IDs = np.arange(2*N)\n",
    "np.random.shuffle(IDs)\n",
    "ID1 = IDs[:N + 1].copy()\n",
    "ID1.sort()\n",
    "df1 = np.floor(pd.DataFrame({\n",
    "\t\"Height (cm)\" : np.random.normal(168, 8, N + 1),\n",
    "\t\"Weight (km)\" : np.random.normal(82, 9, N + 1),\n",
    "}, index=ID1)).astype(\"i\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate more random data with N + 2 rows\n",
    "np.random.shuffle(IDs)\n",
    "ID2 = IDs[:N + 2].copy()\n",
    "ID2.sort()\n",
    "df2 = np.floor(pd.DataFrame({\n",
    "\t\"Age (y)\" : np.random.normal(40, 5, N + 2),\n",
    "\t\"Heart rate (bpm)\" : np.random.normal(78, 9, N + 2),\n",
    "}, index=ID2)).astype(\"i\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Ufuncs on Series, we can compute a BMI column in the first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"BMI\"] = np.round(df1[\"Weight (km)\"] / (df1[\"Height (cm)\"]/100)**2).astype(\"i\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue to, it will be useful to use the following utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen a special case of concatenation, but that had the benefit of having indices the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate the two dataframes to get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to *exclude* those rows that are not complete. We can do this by setting `join=\"inner\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1, join=\"inner\", sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default is `join=\"outer\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example taken from the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-logic-of-the-resulting-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    },\n",
    "    index=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "dfb = pd.DataFrame(\n",
    "    {\n",
    "        \"B\": [\"B2\", \"B3\", \"B6\", \"B7\"],\n",
    "        \"D\": [\"D2\", \"D3\", \"D6\", \"D7\"],\n",
    "        \"F\": [\"F2\", \"F3\", \"F6\", \"F7\"],\n",
    "    },\n",
    "    index=[2, 3, 6, 7],\n",
    ")\n",
    "\n",
    "result = pd.concat([dfa, dfb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('dfa', 'dfb', 'result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging is similar to concatenation, but significantly different. 😅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('dfa', 'dfb', 'dfa.merge(dfb)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike with concatenation, DataFrames are joined by default using `'inner'`, but with the `how` keyword argument.\n",
    "\n",
    "With `merge`, there are many more options. One can set `how` to be:\n",
    "- `inner`\n",
    "- `outer`\n",
    "- `left`\n",
    "- `right`\n",
    "- `cross`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('dfa', 'dfb', 'dfa.merge(dfb, how=\"outer\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the largest set of compatible columns are choosen. \n",
    "\n",
    "We can merge on fewer columns by changing the `on` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('dfa', 'dfb', 'dfa.merge(dfb, how=\"outer\", on=\"B\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the `_x` and `_y` using the `suffixes` keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('dfa', 'dfb', 'dfa.merge(dfb, how=\"outer\", on=\"B\", suffixes=(\"_left\", \"_right\"))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `how='left'` pulls only those rows from the left DataFrame. Similarly for `how='right'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the DataFrames associated to `03_road_deaths.csv` and `09_CO2_combustion.csv`.\n",
    "    - Build a DataFrame (either by concatenation or merging) with the main value from both DataFrames for both sexes (in `03_....csv`) and for the total amount (in `09_....csv`). The time span is up to you, but it should be at least 10 years.\n",
    "    - Determine the average and standard deviation during the time span you determined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
