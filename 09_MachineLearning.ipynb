{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://notebooks.gesis.org/binder/v2/gh/joshmaglione/CS102-Jupyter/main?labpath=.%2FWeek09.ipynb) \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/joshmaglione/CS102-Jupyter/blob/main/Week09.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> \n",
    "\n",
    "[View on GitHub](https://github.com/joshmaglione/CS102-Jupyter/blob/main/Week09.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Introduction to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine learning involves building mathematical models to turn data into *information*.\n",
    "\n",
    "- These models depend on *tunable* parameters that can be adjusted.\n",
    "  \n",
    "- In this way the model can be considered to be 'learning' from the data as it tunes the parameters accordingly.\n",
    "\n",
    "- Once these models have been fit to some data set, sometimes referred to as \"trained\", they can be used to convert data to information on other data sets. \n",
    "\n",
    "- The effectiveness of the model depends many factors, one of which is the size of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `scikit-learn` for much of our ML discussion. \n",
    "\n",
    "There isn't just one tool or one 'ML algorithm'.\n",
    "\n",
    "![](imgs/scikitlearn1.png)\n",
    "\n",
    "![](imgs/scikitlearn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two flavors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two fundamental differences: *supervised* and *unsupervised* 'learning'.\n",
    "\n",
    "This is simply about the learning or training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning occurs when training data is labeled. \n",
    "\n",
    "Main categories of supervised learning:\n",
    "1. Classification\n",
    "2. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples: \n",
    "1. Measurements of different species of Iris are compared against their species labeled to find a pattern (classification).\n",
    "2. Determining a continuous function, so that events can be predicted in the future (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another classification example: \n",
    "\n",
    "Blueberry muffin or chihuahua? \n",
    "\n",
    "![](imgs/blueberry_chihuahua.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning occurs when training data is *not* labeled. \n",
    "\n",
    "Main categories:\n",
    "1. Clustering\n",
    "2. Association\n",
    "3. Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples include:\n",
    "1. MRI scans are searched for problematic areas (clustering).\n",
    "2. 'Market baset analysis' which is the 'customers who bought X also bought Y' (association).\n",
    "3. Obtaining only the relevant information (e.g. 95% of total variance) of a data set (dimension reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the nature of unsupervised learning -- no need for labels -- it is easy to use multiple tools.\n",
    "\n",
    "For example, reduce the dimension then cluster. \n",
    "\n",
    "This has the potential to save lots of time and electricity than just clustering without dimensiojn reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to irises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the Iris dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn \n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll discuss this as we get to it...\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Irises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(iris.target_names[iris.target], name='species')\n",
    "df_labeled = pd.DataFrame(\n",
    "\tiris.data, \n",
    "\tcolumns=iris.feature_names, \n",
    ")\n",
    "df_labeled = pd.concat([ser, df_labeled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get some quick info about the individual species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_labeled\n",
    "\t.query('species == \"setosa\"')\n",
    "\t.describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_sepal = df_labeled.query(\"species == 'setosa'\")['sepal length (cm)']\n",
    "versicolor_sepal = df_labeled.query(\"species == 'versicolor'\")['sepal length (cm)']\n",
    "virginica_sepal = df_labeled.query(\"species == 'virginica'\")['sepal length (cm)']\n",
    "\n",
    "# Three overlayed histograms helps to compare distributions\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=10)\n",
    "\n",
    "# Plots \n",
    "plt.hist(setosa_sepal, label='setosa', **kwargs)\n",
    "plt.hist(versicolor_sepal, label='versicolor', **kwargs)\n",
    "plt.hist(virginica_sepal, label='virginica', **kwargs)\n",
    "plt.xlabel(\"Sepal Length (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title('Sepal length distributions of different species')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of viewing the three separated, let's stack them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepaldata = [setosa_sepal, versicolor_sepal, virginica_sepal]\n",
    "\n",
    "kwargs = dict(histtype='barstacked', density=True, bins=10)\n",
    "\n",
    "plt.hist(sepaldata,  label=['setosa','versicolor','virginica'], **kwargs)\n",
    "plt.xlabel(\"X axis label\")\n",
    "plt.ylabel(\"Y axis label\")\n",
    "plt.legend()\n",
    "plt.title('Sepal length distributions of different species')\n",
    "plt.xlabel(\"Sepal Length (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the plot we did last week comparing lots of pairs of variables? \n",
    "\n",
    "We can do something similar very easily with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(\n",
    "\tdf_labeled, \n",
    "\tc=iris.target, \n",
    "\tfigsize=(15, 15), \n",
    "\tmarker='o', \n",
    "\thist_kwds={'bins': 20, 'alpha':.6, 'edgecolor':'black'}, \n",
    "\ts=60, \n",
    "\talpha=.7\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be able to get the histograms to look nice.\n",
    "\n",
    "I could not figure out an *easy* way to distinguish species. (I'm sure there's a way...)\n",
    "\n",
    "We can use `seaborn` to do this elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('notebook')\n",
    "_ = sns.pairplot(df_labeled, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** an algorithm that, given some measurements of an Iris, tells us whether it is \n",
    "- setosa \n",
    "- versicolor \n",
    "- virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our data into two classes:\n",
    "- training data\n",
    "- validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use training data to ... train our model\n",
    "\n",
    "We use validation data to ... validify our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn already has something for this `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.random.randint(0, 100, size=(8, 3))\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(test_data, test_size=0.25)\t# Try test_size and train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go with an 80/20 split for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_labeled.drop('species', axis=1)\n",
    "y = df_labeled['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Choosing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose to use a decision tree model. \n",
    "\n",
    "Here's an example.\n",
    "\n",
    "(Can you tell it's made by someone from Silicon Valley???)\n",
    "\n",
    "![](https://images.ctfassets.net/wp1lcwdav1p1/4cpLu1KCkDsmNG3up3Hivs/a8e5c327b618b23c8d306bf1a2764cb9/Screen_Shot_2022-07-25_at_12.15.04_PM.png?w=1500&q=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what will happen is that the algorithm will iteratively split the parameter space.\n",
    "\n",
    "For us the iris data set the parameter space is, say, $\\mathbb{R}^4$ or $\\mathbb{R}_{>0}^4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the above example \n",
    "- the whole space is divided into two\n",
    "- then one piece is divided into two\n",
    "\n",
    "In total, three subspaces -- seen as leaves of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=1)\t# Try different values for max_depth\n",
    "clf.fit(X_train, y_train)\t\t\t\t\t# Train the classifier\n",
    "predictions = clf.predict(X_test)\t\t\t# Test the classifier\n",
    "score = accuracy_score(y_test, predictions)\t# Evaluate the classifier\n",
    "print(f\"Accuracy: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so .... what does that mean?\n",
    "\n",
    "We can use a *confusion matrix* to look a little more closely at the performance of our classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "_ = ConfusionMatrixDisplay(\n",
    "\tconfusion_matrix(y_test, predictions),\n",
    "\tdisplay_labels=['setosa', 'versicolor', 'virginica']\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The rows describe the *actual* values.\n",
    "- The columns describe the *predicted* values.\n",
    "- Ideally, everything is on the diagaonl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see weights for each of the variables based on how important they were in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clf.feature_importances_, index=iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's actually look at the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plot_tree(clf, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on the Gini (diversity) index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also called Gini impurity.\n",
    "\n",
    "The idea is that the Gini index is a real number between $0$ and $1$. \n",
    "\n",
    "The precise formula and explanation is not needed for us.\n",
    "\n",
    "- Closer to $0$ means less diverse,\n",
    "- Closer to $1$ means more diverse.\n",
    "\n",
    "(This has nothing to do with the Gini coefficient from economics -- same [Gini](https://en.wikipedia.org/wiki/Corrado_Gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is code taken from `scikit-learn`'s [tutorial on decision trees](https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html). \n",
    "\n",
    "They have a nice way to look at the how the parameter space is cut by the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "# plot_step = 0.02\n",
    "\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    ax = plt.subplot(2, 3, pairidx + 1)\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        ax=ax,\n",
    "        xlabel=iris.feature_names[pair[0]],\n",
    "        ylabel=iris.feature_names[pair[1]],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(\n",
    "            X[idx, 0],\n",
    "            X[idx, 1],\n",
    "            c=color,\n",
    "            label=iris.target_names[i],\n",
    "            edgecolor=\"black\",\n",
    "            s=15,\n",
    "        )\n",
    "\n",
    "plt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "_ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest disadvantages of decision trees is a phenomenon called *overfitting*.\n",
    "\n",
    "Overfitting occurs when the model is fit too closely to particulars of the given data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/2048px-Overfitting.svg.png)\n",
    "\n",
    "- The black line represents a model that is not overfitted.\n",
    "- The green line represents a model that is overfitted.\n",
    "- The red and blue dots with a black outline represents new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is very common with decision trees. \n",
    "\n",
    "There are ways to mitigate this, they aren't perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way mitigate is the following.\n",
    "1. Plant a bunch of decision trees -- grow a decision forest. \n",
    "2. Allow them each to become overfitted.\n",
    "3. Take an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "pred_forest = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred_forest)\n",
    "print(f'Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ConfusionMatrixDisplay(\n",
    "\tconfusion_matrix(y_test, pred_forest),\n",
    "\tdisplay_labels=['setosa', 'versicolor', 'virginica']\n",
    ").plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
