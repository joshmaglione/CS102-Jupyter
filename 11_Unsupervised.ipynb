{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://notebooks.gesis.org/binder/v2/gh/joshmaglione/CS102-Jupyter/main?labpath=.%2F11_Unsupervised.ipynb) \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/joshmaglione/CS102-Jupyter/blob/main/11_Unsupervised.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> \n",
    "\n",
    "[View on GitHub](https://github.com/joshmaglione/CS102-Jupyter/blob/main/11_Unsupervised.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 : Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes\n",
    "By the end of this notebook, you should be able to:\n",
    "- describe what unsupervised learning is and when it is useful\n",
    "- apply dimensionality reduction (e.g., PCA) to simplify or visualize high-dimensional data\n",
    "- interpret unsupervised results (components/clusters) and discuss limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:587/1*J82yf-YU7Ryhh5BdhyOdcw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between supervised and unsupervised learning?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning can highlight intrinsic information about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example is **dimension reduction**:\n",
    "- take high-dimensional data and return lower-dimensional data that still retains most of the information (usually info = total variance)\n",
    "- used for:\n",
    "  - visualization (from 10 dimensions to 2)\n",
    "  - speed up model training (from 2000 dimensions to 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is perhaps one of the most broadly used unsupervised algorithms. \n",
    "\n",
    "(My personal favorite)\n",
    "\n",
    "PCA is fundamentally a **dimensionality reduction** algorithm, but it can be used for (among other things)\n",
    "- visualization\n",
    "- noise filtering\n",
    "- feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematics around PCA is super fun. \n",
    "\n",
    "In [Geometric Foundations for Data Analysis](https://joshmaglione.com/2023CS4102.html) we go through PCA in detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the two principal components, which are just vectors.\n",
    "\n",
    "We'll plot both principal components on the plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"data/pcadata.csv\")\n",
    "\n",
    "# Helper function\n",
    "def draw_vector(v0, v1, color='red'):\n",
    "    ax = plt.gca()\n",
    "    arrowprops = dict(arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0, color=color)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# Perform PCA\n",
    "model = PCA(n_components=2)\n",
    "model.fit(df)\n",
    "\n",
    "# Plot the principal components\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "plt.scatter(df.x, df.y, c='blue', alpha=0.5, zorder=2)\n",
    "mu = model.mean_\n",
    "PC1, PC2 = model.components_\n",
    "var1, var2 = model.explained_variance_\n",
    "draw_vector(mu, mu + 3*PC1, color='red')\n",
    "draw_vector(mu, mu + 3*PC2, color='orange')\n",
    "plt.text(mu[0] + 3*PC1[0] - 1, mu[1] + 3*PC1[1] + 0.5, 'PC1', color='red', fontsize=12, ha='right')\n",
    "plt.text(mu[0] + 3*PC2[0] + 0.5, mu[1] + 3*PC2[1] + 0.75, 'PC2', color='orange', fontsize=12, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=2)\t\t\t\t\t\t# Bringing it here to see\n",
    "model.fit(df)\n",
    "print(f\"First component: PC1 = {model.components_[0]}\")\n",
    "print(f\"Second component: PC2 = {model.components_[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To each principal component is a value -- eigenvalue eigenvector pairs -- this measures variance.\n",
    "\n",
    "The first principal component is where the largest variance occurs. \n",
    "\n",
    "It is decreasing (in order) for the rest of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the data onto the first 2 principal components\n",
    "projected_data = model.transform(df)\n",
    "\n",
    "# Create a scatter plot of the projected data\n",
    "plt.scatter(projected_data[:, 0], projected_data[:, 1], c='blue', alpha=0.5, zorder=2)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Data on First 2 Principal Components')\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that PC1 is not just one variable $x$ or $y$. It is a *linear combination* of the two.\n",
    "\n",
    "This can make it challenging to interpret the components, but for many scenarios this is OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the vector of the explained variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us :\n",
    "- $97.6\\%$ of the variance is seen in PC1\n",
    "- $2.4\\%$ of the variance (the rest) is seen in PC2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So really, we could just keep one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\t# Only use 1 component\n",
    "X = df.to_numpy()\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.25)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.75)\n",
    "plt.title('PCA with 1 Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The light blue points are the original data, while the orange points are the projected version. \n",
    "\n",
    "The key to PCA:\n",
    "- information along the least important principal axes are removed!\n",
    "- Th reduced dataset is, in some sense, good enough to encode the most important relationships between the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a toy example, so it's not easy to see the benefit, but it is easy to see the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up the Iris data set again and plot the basic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import seaborn as sns\n",
    "\n",
    "iris = load_iris()\n",
    "ser = pd.Series(iris.target_names[iris.target], name='species')\n",
    "df_labeled = pd.DataFrame(\n",
    "\tiris.data, \n",
    "\tcolumns=iris.feature_names, \n",
    ")\n",
    "df_labeled = pd.concat([ser, df_labeled], axis=1)\n",
    "\n",
    "_ = sns.pairplot(\n",
    "\tdf_labeled, \n",
    "\thue='species'\n",
    ")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This is just an excuse to enjoy a nice image ðŸ« )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also recall that we can get quick information with `describe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the scale of petal width is much smaller than the other features. \n",
    "\n",
    "This is a problem for PCA, *which is sensitive to the scale of the features*. \n",
    "\n",
    "We can fix this by standardizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_labeled.drop('species', axis=1))\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df_labeled.columns[1:])\n",
    "df_scaled = pd.concat([df_labeled['species'], df_scaled], axis=1)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umm what?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/dAVLtOPb0JeIE/giphy.gif?cid=ecf05e476jqe1aysalqrm12p61ybtfb2bo4aeg27gpsrxng4&ep=v1_gifs_search&rid=giphy.gif&ct=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's alright Troy, let's plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(\n",
    "\tdf_scaled, \n",
    "\thue='species'\n",
    ")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation is not a pure translation, but it does keep most of the \"geometry\" in tact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's actually do the PCA now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X = df_scaled.drop('species', axis=1)\n",
    "X_new = pca.fit_transform(X)\n",
    "X_new=pd.DataFrame(X_new, columns = [f\"Feature {i}\" for i in range(1,5)])\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The explained variance of all 4 components:\")\n",
    "for i, r in enumerate(pca.explained_variance_ratio_):\n",
    "\tprint(f\"PC{i+1}: {r*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would get over $95\\%$ of the total variance in the first two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr(), annot = True, square = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_new.corr(), annot = True, square = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is *the key feature* of PCA:\n",
    "- the principal components are uncorrelated (linearly independent!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are the principal components in this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Principal components:\")\n",
    "for i, row in enumerate(pca.components_):\n",
    "\tprint(f\"PC{i+1}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might interpret \"Feature 1\" as approximately:\n",
    "$$\n",
    "\t\\frac{1}{2}x_{\\text{s. len.}} - \\frac{1}{4} x_{\\text{s. wid.}} + \\frac{1}{2} x_{\\text{p. len.}} + \\frac{1}{2} x_{\\text{p. wid.}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first two features have nearly all of the variances, let's project onto these two dimensions.\n",
    "\n",
    "We'll plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.scatterplot(\n",
    "\tx=X_new['Feature 1'], \n",
    "\ty=X_new['Feature 2'], \n",
    "\thue=df_scaled['species']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just include all 4 dimensions. Four is tiny, but two is nice to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that PCA has not used the labels *and* you can see clusters of species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms group subsets of data based on features.\n",
    "\n",
    "The notion of a \"cluster\" is tricky to define canonically. \n",
    "\n",
    "In one context clusters might be determined by\n",
    "- distance from a centroid\n",
    "- distance from nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many kinds of clustering algorithms. \n",
    "\n",
    "We will look at $k$-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind $k$-means clustering:\n",
    "- Specify $k$ the number of desired clusters\n",
    "- It will find $k$ subsets of the data based on minimal distance to centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our data were in $\\mathbb{R}^2$ and we wanted $k=9$ clusters, data gets partitioned based on the following *tesselation*.\n",
    "\n",
    "![](imgs/Voronoi_growth_euclidean.gif)\n",
    "\n",
    "Any data point in the blue region, for example, would be in the blue cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the `digits` data set in scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "\n",
    "# Printing aspects of the data\n",
    "(n_samples, n_features) = data.shape\n",
    "n_digits = np.unique(labels).size\n",
    "\n",
    "print(f\"Number of digits  : {n_digits}\")\n",
    "print(f\"Number of samples : {n_samples}\")\n",
    "print(f\"Number of features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all ten digits represented in our data set. \n",
    "\n",
    "The number of data points (i.e. image data) is 1,797. \n",
    "\n",
    "Each data point lives in $\\R^{64} \\cong \\R^{8\\times 8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].reshape(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code to sample 240 digits and convert them into an $8\\times 8$ image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = data.reshape(-1, 8, 8)\n",
    "indices = np.random.choice(len(digits), size=240, replace=False)\n",
    "sample_digits = digits[indices]\n",
    "\n",
    "fig, axes = plt.subplots(12, 20, figsize=(10, 6))\n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(\n",
    "        axis='both',\n",
    "        which='both',\n",
    "        bottom=False,\n",
    "        left=False,\n",
    "        labelbottom=False,\n",
    "        labelleft=False\n",
    "    )\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(sample_digits[i], cmap='gray_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think we need all 64 dimensions? Let's build code for PCA, but not use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pca = PCA(n_components=64)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "kmeans = KMeans(n_clusters=n_digits)\n",
    "kmeans.fit(reduced_data)\n",
    "for i, lab in enumerate(np.unique(kmeans.labels_)):\n",
    "\tprint(f\"Cluster {i}: {np.sum(kmeans.labels_ == lab)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more parameters that we can tune for $k$-means, but we're ignoring that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that \"Cluster $k$\" is not necessarily digit $k$.\n",
    "\n",
    "Let's plot everything with the cluster label $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster 0 samples\n",
    "digits0 = data[kmeans.labels_ == 0]\n",
    "\n",
    "fig, axes = plt.subplots(12, 20, figsize=(10, 6))\n",
    "# Turn off everything except the images and border\n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(\n",
    "        axis='both',\n",
    "        which='both',\n",
    "        bottom=False,\n",
    "        left=False,\n",
    "        labelbottom=False,\n",
    "        labelleft=False\n",
    "    )\n",
    "# Plot the images\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(digits0):\n",
    "        ax.imshow(digits0[i].reshape(8, 8), cmap='gray_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We knew how many clusters we wanted because we know there are just 10 digits. \n",
    "\n",
    "But needing to declare the number of clusters at the outset makes this a bit awkward.\n",
    "\n",
    "- For example, the algorithm cannot learn from the data how many clusters there should be.\n",
    "\n",
    "Well... this isn't exactly true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are *method and heuristics* to try to determine the optimal number of clusters.\n",
    "- What is \"optimal\"? ðŸ¤·\n",
    "\n",
    "There are some standard notions of optimal, but they are guidelines more than anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric for this is called *inertia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample\n",
    "- we compute the squared distance to its centroid\n",
    "- then we sum up the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One heuristic is to make an \"elbow plot\" -- at the one hopes it resembles an elbow.\n",
    "\n",
    "- Pick some candidate $k$ values, maybe `range(1, 26)`, and plot the inertia for each.\n",
    "- If the plot sharply decreases as $k$ increases and then suddenly plateaus, you are in luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does *not* happen for the digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inerts = []\n",
    "ks = range(1,21)\n",
    "for k in ks:\n",
    "    kmeans_model = KMeans(n_clusters=k)\n",
    "    kmeans_model.fit(reduced_data)          # Try X (iris data)\n",
    "    inerts.append(kmeans_model.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.grid()\n",
    "plt.plot(ks, inerts, 'bx-')\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method showing the optimal $k$')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to improve the elbow plot, but it still only sometimes works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsides and downsides of $k$-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a helper function to demonstrate $k$-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the Voronoi diagram\n",
    "def Voronoi(data, kmeans=None):\n",
    "    x_min = (data.T)[0].min()\n",
    "    x_max = (data.T)[0].max()\n",
    "    y_min = (data.T)[1].min()\n",
    "    y_max = (data.T)[1].max()\n",
    "    ma = max(x_max, y_max)\n",
    "    mi = min(x_min, y_min)\n",
    "    x_min -= (ma - mi)/20\n",
    "    y_min -= (ma - mi)/20\n",
    "    x_max += (ma - mi)/20\n",
    "    y_max += (ma - mi)/20\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    if kmeans:\n",
    "        incr = 0.01\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.arange(x_min, x_max, incr), \n",
    "            np.arange(y_min, y_max, incr)\n",
    "        )\n",
    "        Z = np.c_[xx.ravel(), yy.ravel()]\n",
    "        kmeans.fit(Z)\n",
    "        L = kmeans.predict(Z)\n",
    "        L = L.reshape(xx.shape)\n",
    "        plt.imshow(\n",
    "            L,\n",
    "            interpolation=\"nearest\",\n",
    "            extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "            cmap=plt.cm.Paired,\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "    plt.tick_params(\n",
    "        left = False, \n",
    "        right = False, \n",
    "        labelleft = False, \n",
    "        labelbottom = False, \n",
    "        bottom = False\n",
    "    )\n",
    "    plt.scatter(data[:,0], data[:,1], c=\"black\", alpha=0.66, s=25)    \n",
    "    if kmeans:\n",
    "        cents = kmeans.cluster_centers_\n",
    "        plt.scatter(cents[:,0], cents[:,1], c=\"white\", marker=\"x\", linewidths=2, s=100)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 data sets for this demonstration:\n",
    "- `three_clusters_clear.csv`\n",
    "- `three_clusters_vague.csv`\n",
    "- `two_clusters_unbalanced.csv`\n",
    "- `impossible.csv`\n",
    "\n",
    "We'll look at the first and third. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/three_clusters_clear.csv\").to_numpy()\n",
    "Voronoi(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's produce a clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "Voronoi(data, kmeans).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more data set to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/two_clusters_unbalanced.csv\").to_numpy()\n",
    "Voronoi(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "Voronoi(data, kmeans).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-means is not the only clustering algorithm out there. \n",
    "\n",
    "There are many more, even ones that are fundamentally different:\n",
    "- heirarchical clustering\n",
    "- topological clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of machine learning:\n",
    "- supervised\n",
    "- unsupervised\n",
    "\n",
    "This corresponds to whether or not the data is labeled.\n",
    "\n",
    "While one particular tool might not be useful in a specific situation, it is important to be familiar with the whole toolbox.\n",
    "\n",
    "We saw algorithms and examples of \n",
    "- decision trees\n",
    "- linear regression\n",
    "- principal component analysis\n",
    "- $k$-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Run through the Iris example with:\n",
    "\t- no scaler\n",
    "\t- the `MinMaxScaler`\n",
    "\n",
    "   what differences do you see? How do the correlation matrices differ? You can build a correlation matrix with the `corr` method: e.g. `df.corr()`.\n",
    "\n",
    "2. Run a $k$-means clustering on `three_clusters_vague.csv` and `impossible.csv`. Feel free to use the helper function `Voronoi(data, kmeans=None)`, where `data` a NumPy array of your data and `kmeans` is the `kmeans` model. If no model is given, a standard plot of the data is returned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
