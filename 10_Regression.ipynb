{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://notebooks.gesis.org/binder/v2/gh/joshmaglione/CS102-Jupyter/main?labpath=.%2F10_Regression.ipynb) \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/joshmaglione/CS102-Jupyter/blob/main/10_Regression.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> \n",
    "\n",
    "[View on GitHub](https://github.com/joshmaglione/CS102-Jupyter/blob/main/10_Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 : Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes\n",
    "By the end of this notebook, you should be able to:\n",
    "- fit a simple regression model with `scikit-learn` and use it to make predictions\n",
    "- visualize regression data and fitted lines using Matplotlib\n",
    "- evaluate regression performance with appropriate summary measures (e.g., $R^2$ and error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue learning about machine learning. We are still looking at *supervised learning*.\n",
    "\n",
    "- What is the difference between *supervised* and *unsupervised* learning? \n",
    "\n",
    "- What are some examples of each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random as rng\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still working with `scikit-learn` which can help us with several machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/scikitlearn1.png)\n",
    "\n",
    "![](imgs/scikitlearn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably already know what linear regression is if the name is unfamiliar.\n",
    "\n",
    "It is used all the time in sciences.\n",
    "\n",
    "Linear regression = finding the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*N1-K-A43_98pYZ27fnupDA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a linear regression when we want to analyze how some variables (usually one) *linearly* depend on other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simplest:** we have data points in $\\mathbb{R}^2$.\n",
    "- We suspect the $y$-values depend, at least in part, on the $x$-values.\n",
    "- We perform some linear algebra to get the line of best fit, usually written as\n",
    "$$\n",
    "\ty = \\beta x + \\varepsilon\n",
    "$$\n",
    "- We calculate the $R^2$-value to see how well the line fits the data.\n",
    "- If it's a decent fit, we can predict a reasonable range of $y$-values for a given $x$-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10 * rng.rand(50) \t\t\t# Random numbers between 0 and 10\n",
    "noise = rng.randn(50)\t\t\t# Random noise\n",
    "y = 2 * x - 5 + noise \t\t\t# Noisy but near the line 2x - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "_ = plt.scatter(x, y, color='blue', alpha=0.5, zorder=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very *clear* trend in the data. We expect a high $r^2$-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `LinearRegression` to get the line of best fit in this simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "xfit = np.linspace(0, 10, 2)\t\t\t# Only need 2 points to define a line\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "beta = model.coef_[0]\t\t\t\t\t# slope\n",
    "epsilon = model.intercept_\t\t\t\t# intercept\n",
    "r2 = model.score(x[:, np.newaxis], y)\t# r^2-value\n",
    "\n",
    "# Plot the model\n",
    "plt.scatter(x, y, color='blue', alpha=0.5, zorder=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.plot(xfit, yfit, color=\"red\", label=f\"$y = {beta:.2f}x {epsilon:.2f}$\")\n",
    "plt.text(0.1, 13, f\"$R^2 = {r2:.2f}$\", fontsize=12, color=\"green\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected a very high $R^2$-value since we produced the data to have very little noise. \n",
    "\n",
    "**Try it yourself:** Scale the vector `noise` above and see how it impacts the $r^2$-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking for a line (of best fit), we will look for a hyperplane.\n",
    "\n",
    "It's the same idea, except we have more \"independent variables\".\n",
    "\n",
    "We will still have one dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gathered some data related to housing from \n",
    "- centralbank.ie\n",
    "- cso.ie\n",
    "\n",
    "#### Information on the data\n",
    "\n",
    "The central bank tells us the interest rates for fixed rate mortgages for each quarter between 2015 and a little bit before 2020. \n",
    "\n",
    "The CSO tells us the number of new and existing houses sold in Ireland each month and their cumulative values. They also give the median house price for all of Ireland."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this all together required some serious work. \n",
    "\n",
    "Below is the code to gather the data from the csv files. \n",
    "\n",
    "This is just for your information -- you do not need to understand this code -- though *you can*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional to read\n",
    "#   I am taking two separate data sets and combining them into one. I need to\n",
    "#   clean the data up a bit in the process.\n",
    "# \tI am sure there are ways to improve this. If you do so, tell me :)\n",
    "\n",
    "# Data from https://www.centralbank.ie/statistics/data-and-analysis/statistical-data-in-csv\n",
    "# Interest data\n",
    "df_int = pd.read_csv(\n",
    "\t\"data/Interest.csv\", \n",
    "\tencoding = \"ISO-8859-2\", \n",
    "\tusecols=[\"Table B.3.1  Retail Interest Rates - Lending for House Purchase\", \n",
    "\t\t\"Reporting Date\", \n",
    "\t\t\"PDH - Fixed Rate - Over 3 years\"\n",
    "\t],\n",
    "\tparse_dates=[\"Reporting Date\"]\n",
    ")\n",
    "df_int = (df_int\n",
    "\t.query(\"`Table B.3.1  Retail Interest Rates - Lending for House Purchase` == 'Outstanding Amounts - Rates (%)'\")\n",
    "\t.drop(columns=[\"Table B.3.1  Retail Interest Rates - Lending for House Purchase\"])\n",
    "\t.rename(columns={\n",
    "\t\t\"Reporting Date\": \"Date\", \n",
    "\t\t\"PDH - Fixed Rate - Over 3 years\": \"Interest\"\n",
    "\t})\n",
    ")\n",
    "def month_to_interest(date):\n",
    "\trel_data = df_int.query(f\"Date <= '{date}'\")\n",
    "\tif rel_data.empty:\n",
    "\t\treturn np.nan\n",
    "\treturn rel_data.tail(1).Interest.values[0]\n",
    "\n",
    "# Data from: https://data.cso.ie/table/HPM04\n",
    "# So much housing data\n",
    "from functools import reduce\n",
    "df = pd.read_csv(\n",
    "\t\"data/Household.csv\",\n",
    "\tparse_dates=[\"Month\"], \n",
    "\tdate_format='%Y %m'\n",
    ")\n",
    "df = df.rename(columns={\"Statistic Label\": \"Label\"})\n",
    "dfs = [df.query(f\"Label == '{x}'\") for x in df.Label.unique()]\n",
    "for i, df in enumerate(dfs):\n",
    "\tdfs[i] = (df\n",
    "\t\t.rename(columns={\"VALUE\": f\"{df.Label.unique()[0]}\"})\n",
    "\t\t.query(\"`Stamp Duty Event` == 'Executions'\")\n",
    "\t\t.drop(columns=[\"Label\", \"UNIT\", \"Stamp Duty Event\", \"Eircode Output\"])\n",
    "\t)\n",
    "df = reduce(pd.merge, dfs)\n",
    "df = (df\n",
    "\t.rename(columns={\n",
    "\t\t\"Volume of Sales\": \"Volume\", \n",
    "\t\t\"Median Price\": \"Median\", \n",
    "\t\t\"Value of Sales\": \"Value\"\n",
    "\t})\n",
    "\t.astype({\"Volume\": \"i\"})\n",
    "\t.query(\"`Type of Buyer` == 'All Buyer Types'\")\n",
    "\t.drop(columns=[\"Type of Buyer\", \"Mean Sale Price\"])\n",
    "\t.set_index(\"Month\")\n",
    ")\n",
    "df_all = df.query(\"`Dwelling Status` == 'All Dwelling Statuses'\")\n",
    "df_new = df.query(\"`Dwelling Status` == 'New'\")\n",
    "df_exist = df.query(\"`Dwelling Status` == 'Existing'\")\n",
    "ser_int = pd.Series([month_to_interest(date) for date in df_new.index], index=df_new.index)\n",
    "df = pd.DataFrame({\n",
    "\t\"Volume_New\": df_new.Volume,\n",
    "\t\"Value_New\": df_new.Value,\n",
    "\t\"Volume_Existing\": df_exist.Volume,\n",
    "\t\"Value_Existing\": df_exist.Value,\n",
    "\t\"Interest\": ser_int,\n",
    "\t\"Median\": df_all.Median\n",
    "}).dropna()\n",
    "# Celebrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alas, our data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the data:\n",
    "\n",
    "- Since interest rate is given by quarters, I just repeated them for the months within.\n",
    "- Column meanings:\n",
    "  - **Volume_New**: Number of new houses sold\n",
    "  - **Value_New**: Approximate total value of new houses sold in millions of Euro\n",
    "  - **Volume_Existsing**: Number of houses sold that were previous owned (now called \"existing houses\")\n",
    "  - **Value_Existing**: Approximate total value of existing houses sold in millions of Euro\n",
    "  - **Interest**: The interest rate for fixed mortgages for more than three years\n",
    "  - **Meadian**: The median sale price for a house in Ireland "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Won't go into the details, but the correlation between two variables, say, $x$ and $y$ is a value between $-1$ and $1$.\n",
    "\n",
    "Correlation measures *linear* dependency. For example $y$ *depends linearly* on $x$ if there are real numbers $\\alpha$ and $\\beta$ such that\n",
    "$$\n",
    "\ty = \\alpha x + \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, variables may have many \"factors\" that influence their value. \n",
    "\n",
    "Loosely speaking, suppose \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tx &= a_1 + \\cdots + a_m + c_1 + \\cdots + c_{\\ell}, \\\\\n",
    "\ty &= b_1 + \\cdots + b_n + c_1 + \\cdots + c_{\\ell}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here \"$=$\" means \"is influenced by\". \n",
    "\n",
    "The more factors in common (e.g. the $c_i$) the larger the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, $x$ and $y$ are *postiviely correlated* if an increase in one implies an increase in the other (everything else fixed).\n",
    "\n",
    "And they are *negatively correlated* if an increase in one implies a decrease in the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corrmat = df.corr()\n",
    "sns.heatmap(corrmat, annot = True, square = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply linear regression on the data. \n",
    "\n",
    "We'll look specifically at the two volume columns and the interest column.\n",
    "\n",
    "Can we predict the Median column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Columns to use as dependent variables\n",
    "COLS = df.columns[[0, 2, 4]]\n",
    "\n",
    "X = df[COLS]\n",
    "y = df['Median']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MODEL DETAILS\")\n",
    "print(\"Variables:\")\n",
    "for i, col in enumerate(COLS.values):\n",
    "\tprint(f\"\\tx_{i} = {col}\")\n",
    "print(f\"\\ty   = {y.name}\")\n",
    "print(f\"Terms:\\n\\tepsilon = {model.intercept_:,.2f}\")\n",
    "for i, coef in enumerate(model.coef_):\n",
    "\tprint(f\"\\tbeta_{i}  = {coef:,.2f}\")\n",
    "print(f\"R^2-value: {model.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above data, we get a linear function of the form:\n",
    "$$\n",
    "\ty = \\beta_0 x_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n + \\epsilon. \n",
    "$$\n",
    "\n",
    "And the $r^2$-value tells us how well the above equation fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek balance.\n",
    "\n",
    "![](https://ih1.redbubble.net/image.475188184.1134/flat,750x,075,f-pad,750x1000,f8f8f8.u3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is one of the most important concepts in machine learning.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few examples, I will randomly generate some data.\n",
    "\n",
    "Although it is not \"real-world\" data, the phenomena occur in the real-world. \n",
    "\n",
    "It can be hard sometimes to see the signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can skip over this. \n",
    "# I am building data for the next figure.\n",
    "\n",
    "# Some helper functions for the next few pictures\n",
    "def make_data(N=30, err=0.8, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))\n",
    "\n",
    "# fig0 ------------------------------------------------------------------------\n",
    "X, y = make_data()\n",
    "xfit = np.linspace(-0.1, 1.0, 1000)[:, None]\n",
    "model1 = PolynomialRegression(1).fit(X, y)\t\t# A high-bias model\n",
    "model20 = PolynomialRegression(20).fit(X, y)\t# A high variance model\n",
    "\n",
    "fig0, ax0 = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig0.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "ax0[0].scatter(X.ravel(), y, s=40, color='blue', alpha=0.5)\n",
    "ax0[0].plot(xfit.ravel(), model1.predict(xfit), color='red')\n",
    "ax0[0].axis([-0.1, 1.0, -2, 14])\n",
    "ax0[0].set_title('High-bias model: Underfits the data', size=14)\n",
    "\n",
    "ax0[1].scatter(X.ravel(), y, s=40, color='blue', alpha=0.5)\n",
    "ax0[1].plot(xfit.ravel(), model20.predict(xfit), color='red')\n",
    "ax0[1].axis([-0.1, 1.0, -2, 14])\n",
    "ax0[1].set_title('High-variance model: Overfits the data', size=14)\n",
    "plt.close(fig0)\t\t\t\t\t\t\t\t\t# Shh, not yet!\n",
    "\n",
    "# fig1 -------------------------------------------------------------------------\n",
    "X2, y2 = make_data(10, rseed=42)\n",
    "\n",
    "fig1, ax1 = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig1.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "ax1[0].scatter(X.ravel(), y, s=40, c='blue', alpha=0.5)\n",
    "ax1[0].plot(xfit.ravel(), model1.predict(xfit), color='red')\n",
    "ax1[0].axis([-0.1, 1.0, -2, 14])\n",
    "ax1[0].set_title('High-bias model: Underfits the data', size=14)\n",
    "ax1[0].scatter(X2.ravel(), y2, s=40, c='green', alpha=0.5)\n",
    "ax1[0].text(\n",
    "\t0.02, \n",
    "\t0.98, \n",
    "\tf\"training score: $R^2$ = {model1.score(X, y):.2f}\",\n",
    "\tha='left', \n",
    "\tva='top', \n",
    "\ttransform=ax1[0].transAxes, \n",
    "\tsize=14, \n",
    "\tcolor='blue'\n",
    ")\n",
    "ax1[0].text(\n",
    "\t0.02, \n",
    "\t0.91, \n",
    "\tf\"validation score: $R^2$ = {model1.score(X2, y2):.2f}\",\n",
    "\tha='left', \n",
    "\tva='top', \n",
    "\ttransform=ax1[0].transAxes, \n",
    "\tsize=14, \n",
    "\tcolor='green'\n",
    ")\n",
    "\n",
    "ax1[1].scatter(X.ravel(), y, s=40, c='blue', alpha=0.5)\n",
    "ax1[1].plot(xfit.ravel(), model20.predict(xfit), color='red')\n",
    "ax1[1].axis([-0.1, 1.0, -2, 14])\n",
    "ax1[1].set_title('High-variance model: Overfits the data', size=14)\n",
    "ax1[1].scatter(X2.ravel(), y2, s=40, c='green', alpha=0.5)\n",
    "ax1[1].text(\n",
    "\t0.02, \n",
    "\t0.98, \n",
    "\tf\"training score: $R^2$ = {model20.score(X, y):.2f}\",\n",
    "    ha='left', \n",
    "\tva='top', \n",
    "\ttransform=ax1[1].transAxes, \n",
    "\tsize=14, \n",
    "\tcolor='blue'\n",
    ")\n",
    "ax1[1].text(\n",
    "\t0.02, \n",
    "\t0.91, \n",
    "\tf\"validation score: $R^2$ = {model20.score(X2, y2):,.2f}\",\n",
    "    ha='left', \n",
    "\tva='top', \n",
    "\ttransform=ax1[1].transAxes, \n",
    "\tsize=14, \n",
    "\tcolor='green'\n",
    ")\n",
    "plt.close(fig1)\n",
    "\n",
    "# fig2 -------------------------------------------------------------------------\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y1 = -(x - 0.5) ** 2\n",
    "y2 = y1 - 0.33 + np.exp(x - 1)\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(x, y2, lw=10, alpha=0.5, color='blue')\n",
    "ax2.plot(x, y1, lw=10, alpha=0.5, color='red')\n",
    "ax2.text(0.15, 0.2, \"training score\", rotation=45, size=16, color='blue')\n",
    "ax2.text(0.2, -0.05, \"validation score\", rotation=20, size=16, color='red')\n",
    "ax2.text(\n",
    "\t0.02, \n",
    "\t0.1, \n",
    "\t'$\\\\longleftarrow$ High Bias', \n",
    "\tsize=18, \n",
    "\trotation=90, \n",
    "\tva='center'\n",
    ")\n",
    "ax2.text(\n",
    "\t0.98, \n",
    "\t0.1, \n",
    "\t'$\\\\longleftarrow$ High Variance $\\\\longrightarrow$', \n",
    "\tsize=18, \n",
    "\trotation=90, \n",
    "\tha='right', \n",
    "\tva='center'\n",
    ")\n",
    "ax2.text(\n",
    "\t0.48, \n",
    "\t-0.12, \n",
    "\t'Best$\\\\longrightarrow$\\nModel', \n",
    "\tsize=18, \n",
    "\trotation=90, \n",
    "\tva='center'\n",
    ")\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(-0.3, 0.5)\n",
    "ax2.set_xlabel('model complexity $\\\\longrightarrow$', size=14)\n",
    "ax2.set_ylabel('model score $\\\\longrightarrow$', size=14)\n",
    "ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax2.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax2.set_title(\"Validation Curve Schematic\", size=16)\n",
    "\n",
    "plt.close(fig2)\t\t\t\t\t\t\t\t\t# Shh, not yet!\n",
    "\n",
    "# fig3 -------------------------------------------------------------------------\n",
    "N = np.linspace(0, 1, 1000)\n",
    "y1 = 0.75 + 0.2 * np.exp(-4 * N)\n",
    "y2 = 0.7 - 0.6 * np.exp(-4 * N)\n",
    "\n",
    "fig3, ax3 = plt.subplots()\n",
    "\n",
    "ax3.plot(x, y1, lw=10, alpha=0.5, color='blue')\n",
    "ax3.plot(x, y2, lw=10, alpha=0.5, color='red')\n",
    "ax3.text(0.2, 0.88, \"training score\", rotation=-10, size=16, color='blue')\n",
    "ax3.text(0.2, 0.5, \"validation score\", rotation=30, size=16, color='red')\n",
    "ax3.text(\n",
    "\t0.98, \n",
    "\t0.45, \n",
    "\t'Good Fit $\\\\longrightarrow$', \n",
    "\tsize=18, \n",
    "\trotation=90, \n",
    "\tha='right', \n",
    "\tva='center'\n",
    ")\n",
    "ax3.text(\n",
    "\t0.02, \n",
    "\t0.57, \n",
    "\t'$\\\\longleftarrow$ High Variance $\\\\longrightarrow$', \n",
    "\tsize=18, \n",
    "\trotation=90, \n",
    "\tva='center'\n",
    ")\n",
    "ax3.set_xlim(0, 1)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.set_xlabel('training set size $\\\\longrightarrow$', size=14)\n",
    "ax3.set_ylabel('model score $\\\\longrightarrow$', size=14)\n",
    "ax3.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax3.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax3.set_title(\"Learning Curve Schematic\", size=16)\n",
    "plt.close(fig3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balance we seek in life is between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models have errors and these come from three main sources:\n",
    "1. Variance,\n",
    "2. Bias,\n",
    "3. Noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance** measures how much the model (e.g. a classifier) varies as it trains on different data sets.\n",
    "\n",
    "**Bias** measures how much the *expected* model differs from the *expected* output label.\n",
    "\n",
    "**Noise** measures how much the output label differs on different data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will not focus on noise here. \n",
    "- Bias and variance concern the model; noise is all about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example scenarios\n",
    "\n",
    "(These are not exhaustive... these errors are more subtle.)\n",
    "\n",
    "A model has **high bias** when the model is too simple. \n",
    "- Suppose the data is inherently nonlinear but the model is a linear regression. No amount of extra training data will fix this problem.\n",
    "\n",
    "A model has **high variance** when the model is overly complicated.\n",
    "- Overfitting exemplifies this. A degree $100$ polynomial has an $r^2$ of $0.999997$, but on new data it performs very poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a great illustration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data, this might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little bit about $R^2$-values\n",
    "\n",
    "- Used in regression models to measure how well the function fits the data.\n",
    "- $R^2$ is a value between $-\\infty$ and $1$:\n",
    "  - $R^2=1$ means the function perfectly models the data.\n",
    "  - $R^2=0$ means the function simply gives the mean.\n",
    "  - $R^2=-\\infty$ means üò¨‚ò†Ô∏è\n",
    "\n",
    "\n",
    "- For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.\n",
    "- For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a knob to control the complexity of our model -- the degree of the polynomial\n",
    "\n",
    "When we do that and plot the results we get the following behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training score is everywhere higher than the validation score.\n",
    "- For very low model complexity, the training data is underfit, which means that the model is a poor predictor both for the training data and for any previously unseen data.\n",
    "- For very high model complexity, the training data is overfit, which means that the model predicts the training data very well, but fails for any previously unseen data.\n",
    "- For some intermediate value, the validation curve has a maximum. This level of complexity indicates a **suitable trade-off between bias and variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect of model complexity is *the optimal model will generally depend on the size of your training data.*\n",
    "\n",
    "A plot of the training/validation score with respect to the size of the training set is known as a *learning curve*.\n",
    "- A model of a given complexity will **overfit a small dataset**: \n",
    "  - training score is high, validation score is low.\n",
    "- A model of a given complexity will **underfit a large dataset**: \n",
    "  - training score decreases, validation score increases.\n",
    "\n",
    "In both cases, validation score generally won't be better than training score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, models generally have more than one knob to turn. \n",
    "\n",
    "Thus, plots of validation and learning curves change from curves to surfaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is a class of algorithms for classification. \n",
    "\n",
    "The *parameters* would be the intercept and coefficients of the model. \n",
    "- These are somewhat internal to the particular model.\n",
    "\n",
    "The *hyperparameters* are parameters that control the model or learning process. \n",
    "- These are parameters \"outside\" of the model. \n",
    "- Examples are \n",
    "  - the train-test split ratio\n",
    "  - the degree of the polynomial for polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn gives us a way to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_data(40)\n",
    "_ = plt.scatter(X.ravel(), y, c='blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we build a \"parameter grid\".\n",
    "- All the possible combinations of parameters to test\n",
    "- The best set of parameters wins.\n",
    "\n",
    "(There are ways to fine-tune even *this* process, but I'm refraining.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = [{\n",
    "\t'poly__degree': range(1, 10), \n",
    "\t'ridge__alpha': [0.1, 1, 10, 100]\n",
    "\t}]\n",
    "pipeline = Pipeline(steps=[\n",
    "\t('poly', PolynomialFeatures()), \n",
    "\t('ridge', Ridge())\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a *ridge* regression rather than the simpler linear regression. \n",
    "\n",
    "It's roughly the same idea, but with a way to balance the results. \n",
    "\n",
    "The parameter 'alpha' controls this balance. \n",
    "\n",
    "[Read more about it](https://en.wikipedia.org/wiki/Ridge_regression) if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depict our model as a line\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "plt.scatter(X.ravel(), y, c='blue', alpha=0.5)\n",
    "lim = plt.axis()\n",
    "plt.axis(lim)\n",
    "y_test = model.fit(X, y).predict(X_test)\n",
    "plt.text(0.02, 0.98, f\"$R^2$ = {model.score(X, y):.2f}\", ha='left', va='top', transform=plt.gca().transAxes, size=12)\n",
    "plt.text(0.2, 0.1, f\"Equation: $y = {model.named_steps['ridge'].coef_[-1]:.2f}x^{grid_search.best_params_['poly__degree']} + $ lower degree terms\", ha='left', va='top', transform=plt.gca().transAxes, size=12)\n",
    "_ = plt.plot(X_test.ravel(), y_test, c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scikit learn comes with a dataset for [California house prices](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html). Try building a regression model on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
