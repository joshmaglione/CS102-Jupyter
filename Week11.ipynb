{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/joshmaglione/CS102-Jupyter/main?labpath=.%2FWeek11.ipynb) \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/joshmaglione/CS102-Jupyter/blob/main/Week11.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> \n",
    "\n",
    "[View on GitHub](https://github.com/joshmaglione/CS102-Jupyter/blob/main/Week11.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:587/1*J82yf-YU7Ryhh5BdhyOdcw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between supervised and unsupervised learning?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning can highlight intrinsic information about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example is **dimension reduction**:\n",
    "- take high-dimensional data and return lower-dimensional data that still retains most of the information (usually info = total variance)\n",
    "- used for:\n",
    "  - visualization (from 10 dimensions to 2)\n",
    "  - speed up model training (from 2000 dimensions to 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is perhaps one of the most broadly used unsupervised algorithms. \n",
    "\n",
    "(My personal favorite)\n",
    "\n",
    "PCA is fundamentally a **dimensionality reduction** algorithm, but it can be used for (among other things)\n",
    "- visualization\n",
    "- noise filtering\n",
    "- feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematics around PCA is super fun. \n",
    "\n",
    "In [Geometric Foundations for Data Analysis](https://joshmaglione.com/2023CS4102.html) we go through PCA in detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the two principal components, which are just vectors.\n",
    "\n",
    "We'll plot both principal components on the plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"data/pcadata.csv\")\n",
    "\n",
    "# Helper function\n",
    "def draw_vector(v0, v1, color='red'):\n",
    "    ax = plt.gca()\n",
    "    arrowprops = dict(arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0, color=color)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# Perform PCA\n",
    "model = PCA(n_components=2)\n",
    "model.fit(df)\n",
    "\n",
    "# Plot the principal components\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "plt.scatter(df.x, df.y, c='blue', alpha=0.5, zorder=2)\n",
    "mu = model.mean_\n",
    "PC1, PC2 = model.components_\n",
    "var1, var2 = model.explained_variance_\n",
    "draw_vector(mu, mu + 3*PC1, color='red')\n",
    "draw_vector(mu, mu + 3*PC2, color='orange')\n",
    "plt.text(mu[0] + 3*PC1[0] - 1, mu[1] + 3*PC1[1] + 0.5, 'PC1', color='red', fontsize=12, ha='right')\n",
    "plt.text(mu[0] + 3*PC2[0] + 0.5, mu[1] + 3*PC2[1] + 0.75, 'PC2', color='orange', fontsize=12, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=2)\t\t\t\t\t\t# Bringing it here to see\n",
    "model.fit(df)\n",
    "print(f\"First component: PC1 = {model.components_[0]}\")\n",
    "print(f\"Second component: PC2 = {model.components_[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To each principal component is a value -- eigenvalue eigenvector pairs -- this measures variance.\n",
    "\n",
    "The first principal component is where the largest variance occurs. \n",
    "\n",
    "It is decreasing (in order) for the rest of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the data onto the first 2 principal components\n",
    "projected_data = model.transform(df)\n",
    "\n",
    "# Create a scatter plot of the projected data\n",
    "plt.scatter(projected_data[:, 0], projected_data[:, 1], c='blue', alpha=0.5, zorder=2)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Data on First 2 Principal Components')\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that PC1 is not just one variable $x$ or $y$. It is a *linear combination* of the two.\n",
    "\n",
    "This can make it challenging to interpret the components, but for many scenarios this is OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the vector of the explained variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us :\n",
    "- $97.6\\%$ of the variance is seen in PC1\n",
    "- $2.4\\%$ of the variance (the rest) is seen in PC2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So really, we could just keep one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\t# Only use 1 component\n",
    "X = df.to_numpy()\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.25)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.75)\n",
    "plt.title('PCA with 1 Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The light blue points are the original data, while the orange points are the projected version. \n",
    "\n",
    "The key to PCA:\n",
    "- information along the least important principal axes are removed!\n",
    "- Th reduced dataset is, in some sense, good enough to encode the most important relationships between the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a toy example, so it's not easy to see the benefit, but it is easy to see the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up the Iris data set again and plot the basic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import seaborn as sns\n",
    "\n",
    "iris = load_iris()\n",
    "ser = pd.Series(iris.target_names[iris.target], name='species')\n",
    "df_labeled = pd.DataFrame(\n",
    "\tiris.data, \n",
    "\tcolumns=iris.feature_names, \n",
    ")\n",
    "df_labeled = pd.concat([ser, df_labeled], axis=1)\n",
    "\n",
    "_ = sns.pairplot(\n",
    "\tdf_labeled, \n",
    "\thue='species'\n",
    ")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This is just an excuse to enjoy a nice image ðŸ« )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the scale of petal width is much smaller than the other features. \n",
    "\n",
    "This is a problem for PCA, *which is sensitive to the scale of the features*. \n",
    "\n",
    "We can fix this by standardizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_labeled.drop('species', axis=1))\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df_labeled.columns[1:])\n",
    "df_scaled = pd.concat([df_labeled['species'], df_scaled], axis=1)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umm what?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/dAVLtOPb0JeIE/giphy.gif?cid=ecf05e476jqe1aysalqrm12p61ybtfb2bo4aeg27gpsrxng4&ep=v1_gifs_search&rid=giphy.gif&ct=g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
